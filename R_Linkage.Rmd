---
title: "Record Linkage using R"  
author: "Jared Parrish, PhD"
date: "`r Sys.Date()`"
output: 
    html_document:
      css: styles.css
      toc: true
      toc_float: true
      toc_collapsed: false
      smooth_scroll: false
      number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<div class="logo">
```{r, echo=FALSE, out.width='100px'}
knitr::include_graphics("image/RSImage-329695.png")
```
</div>

---

# Background  
Record linkage is the process of identifying and merging records that refer to the same entity across different data sets. This is particularly useful in scenarios where unique identifiers are not available or reliable. In R, two popular packages for performing record linkage are [RecordLinkage](https://cran.r-project.org/web/packages/RecordLinkage/index.html) and [FastLink](https://cran.r-project.org/web/packages/fastLink/index.html).  

The source code used in this document is available at:  [https://github.com/parrish-epi/R-recordLinkage](https://github.com/parrish-epi/R-recordLinkage)  

## RecordLinkage package  
The [RecordLinkage package](https://journal.r-project.org/archive/2010-2/RJournal_2010-2_Sariyar+Borg.pdf) provides tools for probabilistic record linkage and deduplication. It allows users to compare records based on various attributes, calculate similarity scores, and determine matches based on thresholds. Some of the key features included in this package are:  

* **Pairwise Comparison:** Compares all pairs of records from different data sets or within the same dataset.  
* **Similarity Measures:** Utilizes various similarity measures such as Jaro-Winkler, Levenshtein distance, and Soundex for string comparisons.  
* **Blocking:** Reduces the number of comparisons by grouping records into blocks based on certain criteria, thereby improving efficiency.   
* **Probabilistic Matching:** Implements the Fellegi-Sunter model for probabilistic record linkage and other methods, which calculates the likelihood of matches based on attribute similarities.  
* **Other Features:** Both supervised and unsupervised machine learning methods, manual review match status editing, optimized threshold review specification (e.g., ParetoThreshold), and other linkage tuning and assessment tools.  

## FastLink package  
The fastLink package is designed for large-scale record linkage tasks and focuses on scalability and speed. It extends the functionalities of traditional probabilistic linkage methods and provides tools for estimating and visualizing the linkage process. Some of the key features included in this package are:  

* **Scalability:** Efficiently handles large data sets with millions of records by leveraging parallel computing.  
* **Advanced Blocking:** Implements multiple blocking schemes to ensure that the linkage process remains computationally feasible without sacrificing accuracy.  
* **Unsupervised Learning:** Automatically estimates parameters required for record linkage without the need for training data.  
* **Visualization Tools:** Provides tools for visualizing the linkage results and understanding the matching process.   

Both RecordLinkage and fastLink packages offer robust solutions for record linkage tasks in R. While RecordLinkage is more flexible and offers a variety of similarity measures and probabilistic matching, fastLink is optimized for performance and scalability, making it suitable for large data sets.

This exercise will focus on and provide an example using the RecordLinkage package as the approach is largely extendable to the fastLink package. For those interested in learning and using the fastLink package, check out these resources:   
* [An introduction to fastLink for probabilistic record linkage](https://www.naaccr.org/wp-content/uploads/2018/07/An-Introduction-to-Faslink-for-Probabilistic-Record-Linkage-Alexandersson.pdf)  
* [R package fastLink: Fast Probabilistic Record Linkage](https://github.com/kosukeimai/fastLink)  

The example below demonstrates one approach to using the RecordLinkage package to perform record linkage in R. This includes loading data, pre-processing (cleaning) the data, comparing records, and identifying matches. While this example is not exhaustive and does not cover all the features provided by the package, it serves as a solid starting point for conducting record linkages. 


# Linkage set-up

Prior to any linkage project, it is important to learn about each data set, elements contained within them, and have context for population distributions in the records being linked.  It is also critical that you have your linkage purpose and expectation well defined.   

The code and output below assume at a least a basic understanding of R, for example how to install and load packages.  The data sets used for this exercise were generated using two different generative models (ChatGPT and Google Gemini) and do not reflect actual individuals. 

For this example, the data sets were created to represent the real-life scenario where a health agency draws a sample from a birth record (dat2) and then links those sampled birth records to a population source (dat2) (e.g., hospital discharge records) where the individual may have multiple visits. The expectation is that we will link and retain all individuals from the sample and only the matching population records. This is important to establish up front because during the linkage process, we will identify the matching records and need to be sure to retain all non-matches from the sample (their values will be NULL/NA for all elements in the population source).  

We first need to load a few packages and data for this exercise. If you don't have these packages, you'll need to install the libraries. This exercise was completed using R version 4.4.0.

**Load Packages**
```{R, echo=TRUE, message=FALSE, warning=FALSE}
library(RecordLinkage)
library(stringdist) # for example with cooperators. 

library(dplyr)
library(tidyr)
library(lubridate)  # working with dates
library(stringi)    # for cleaning strings

library(knitr)      # for printing basic tables in the markdown document
library(kableExtra) # for improved tables in the markdown document
library(ggplot2)    # for plotting

library(summarytools) # For summarizing data frames
library(skimr)        # for summarizing data frames 

# Shut off scientific notation, this is critical for long numeric ID's
options(scipen=999) 
```  

**Install data** 
```{R, echo=TRUE, message=FALSE, warning=FALSE}
# Read in data sets to be linked for this example 
# Population source
git1<-'https://raw.githubusercontent.com/parrish-epi/R-recordLinkage/main/SourceA.csv' 
dat1<-read.csv(git1, fileEncoding = "ISO-8859-1") 

# Sample source
git2<-'https://raw.githubusercontent.com/parrish-epi/R-recordLinkage/main/SourceB.csv' 
dat2<-read.csv(git2, fileEncoding = "ISO-8859-1") 

# add the data set to the start of the ID for convenience. 
dat1$UNID <- paste0("dat1_",dat1$UNID)
dat2$ID_source <- paste0("dat2_",dat2$ID_source)

#remove url sets 
rm('git1','git2') 
```

## Review data sets
After the data have been loaded, the first step is to inspect and review the data. During this process, we want to make sure that the data elements to be used for linkages (partial identifiers) are all formatted the same and all column names are identical. For these examples we have two data sets: dat1 and dat2.  

dat1 has `r nrow(dat1)` records and `r ncol(dat1)` variables.  
dat2 has `r nrow(dat2)` records and `r ncol(dat2)` variables.  

Let's first start by checking what names are in each of the data sets.  

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# View contents of data set 1
names(dat1) 
# View contents of data set 2
names(dat2) 
```

## Review data structure
The base r function str() is helpful for viewing the contents of the data but dplyr::glimpse is also useful. 
```{r, echo=TRUE, message=FALSE, warning=FALSE}
# View contents of data set 1
str(dat1) 
# View contents of data set 2
str(dat2) 
```

**Align feature order**  
For the RecordLinkage package we need to align the elements. In this exercise we will align dat1 variable order to dat2.  

```{r, echo=TRUE, warning=FALSE, message=FALSE}
# Making the order of the variables in the data set the same.
dat1 <- dat1[,c(1,8,3,4,2,6,5,7,9)]
str(dat1)
```

**Identify duplicates**
```{r, echo=TRUE, warning=FALSE, message=FALSE}
# Duplicates in dat1
table(duplicated(dat1$UNID)) 

# Duplicates in dat2
table(duplicated(dat2$ID_source))
```


## Align data sets  
Next, we are going to align dat1 variable names and structure to that of dat2. Remember that the variable names, order, and structure must be the same in both data sets for the RecordLinkage package.

**Modify dat1**
```{r, echo=TRUE, warning=FALSE, message=FALSE}
dat1a <- dat1 %>%
  # ensure names of variables are the same between the two data sets.
  rename(
    GivenName = First.Name,
    MiddleName = Middle.Name,
    FamilyName = Last.Name,
    DOB = Date.of.Birth,
    ID_source = UNID
  ) %>%
  # ensure coding of variables is the same between the two data sets.
  # perform basic data cleaning (up case, remove white space and spcial characters)
  mutate(
    Sex = ifelse(Sex == "Female", "F", ifelse(Sex == "Male", "M", NA)),
    DOB = mdy(DOB),
    across(contains("Name"), ~ toupper(.)),
    across(everything(), stringi::stri_trim),
    GivenName = gsub("[[:punct:][:blank:]]+", "", GivenName),
    MiddleName = gsub("[[:punct:][:blank:]]+", "", MiddleName),
    FamilyName = gsub("[[:punct:][:blank:]]+", "", FamilyName),
    across(c("GivenName", "MiddleName", "FamilyName"), 
           ~ stringi::stri_trans_general(str = ., id = "Latin-ASCII")),
    # Break apart date if needing to block to facilitate iterative block linking.
    DOB_YR = as.integer(year(DOB)),
    DOB_MO = as.integer(month(DOB)),
    DOB_DY = as.integer(day(DOB))
    ) %>%
 # Remove indicator specifying correct linkages (part of the test data for checking
#  accuracy but will not have this in a real data linkage)
 select(-InSample)
```

**De-duplicate Records**  
It is easiest to link de-duplicated records. This reduces redundancy and improves the estimated probability scores. It is important in a duplicated data set that represents unique events (e.g., hospital visits) to have an individual unique ID representing the patient, and a unique record level ID representing the visit (event).  

```{r, echo=TRUE, warning=FALSE, message=FALSE}
dat1a.dedup <- dat1a %>% unique()
```


**Modify dat2**
```{r, echo=TRUE, warning=FALSE, message=FALSE}
dat2a <- dat2 %>%
  # conduct the same data cleaning for dat2 as completed in dat1.
  mutate(across(everything(), stringi::stri_trim)) %>%
  mutate(
    DOB_YR = as.integer(year(DOB)),
    DOB_MO = as.integer(month(DOB)),
    DOB_DY = as.integer(day(DOB)),
    across(c("GivenName", "MiddleName", "FamilyName"), 
           ~ gsub("[[:punct:][:blank:]]+", "", .)),
    GivenName = stringi::stri_trans_general(str = GivenName, id = "Latin-ASCII"),
    MiddleName = stringi::stri_trans_general(str = MiddleName, id = "Latin-ASCII"),
    FamilyName = stringi::stri_trans_general(str = FamilyName, id = "Latin-ASCII")
  )
```

## Review aligned data sets

**View contents of the two data sets**  
Here we will use the dplyr::glimpse() function so we can quickly see the number of rows and columns as well.  
```{r, echo=TRUE, warning=FALSE, message=FALSE}
# data structure for dat1 that has been cleaned and de-duplicated.
dplyr::glimpse(dat1a.dedup)

# data structure for dat2 that has been cleaned.
dplyr::glimpse(dat2a)
```

Data preparation is critical for linkages. Before we move forward with the actual linkages let's first inspect the first five records of each data set.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# data set 1
knitr::kable(dat1a.dedup %>% 
             arrange(FamilyName) %>%
             head(5), booktabs = TRUE) %>%
       kableExtra::kable_styling(latex_options = c("scale_down"))

# data set 2
knitr::kable(dat2a %>% 
             arrange(FamilyName) %>%
             head(5), booktabs = TRUE) %>%
       kableExtra::kable_styling(latex_options = c("scale_down"))
```

Finally, the skimr() package provides a really nice tool for reviewing the data elements in the data frame.   

**Dat1 - normalized and de-duplicated**
```{r, echo=TRUE, warning=FALSE, message=FALSE}
skimr::skim(dat1a.dedup)
```

**Dat2 - normalized**

```{r, echo=TRUE, warning=FALSE, message=FALSE}
skimr::skim(dat2a)
```

# Data linkage  
Now that the two data sets to be combined are cleaned in the same way (normalized) and have the same naming and ordering of variables (standardized) we are ready to begin our linkages. The figure below provides a basic linkage flow diagram outlining the major points of a linkage project. 

```{r echo=FALSE, fig.alt="General data linkage flow diagram.", out.width = '100%'}
knitr::include_graphics(
  "C:/Users/parri/Documents/R_Code/RecordLinkage/R_Linkage/image/LinkFlow.png"
  )
```

## Identifier assessment  

It is helpful to start by reviewing how many duplicate records have the same combination of identifiers. This will help establish how "unique" the combination of identifiers are and what identifiers are needed to support accurate results.  

For this exercise we will use the GivenName, MiddleName, FamilyName, Sex, and DOB for our linkages, so we'll look at that first. We'll then look at the combination uniqueness without DOB and then with just year of birth and then day of birth. This exercise is just to demonstrate the investigations that can be done to get an idea of the types of linkages you can expect with the combination of identifiers to be used. 

**Reivew of dat 1 identifiers**

```{r, echo=TRUE, warning=FALSE, message=FALSE}
# start by creating a little function to summarize and view each variable

# based on GivenName, FamilyName, Sex, and DOB
dat1a.dedup %>% group_by(GivenName,FamilyName,Sex,DOB) %>%
                summarise(Count = n()) %>% 
                ungroup() %>%
                group_by(Count) %>%
                summarise(Dist = n())
# based on GivenName, FamilyName, and Sex
dat1a.dedup %>% group_by(GivenName,FamilyName,Sex) %>%
                summarise(Count = n()) %>% 
                ungroup() %>%
                group_by(Count) %>%
                summarise(Dist = n())
# based on GivenName, FamilyName, Sex, and year
dat1a.dedup %>% group_by(GivenName,FamilyName,Sex, DOB_YR) %>%
                summarise(Count = n()) %>% 
                ungroup() %>%
                group_by(Count) %>%
                summarise(Dist = n())
# based on GivenName, FamilyName, Sex, and month
dat1a.dedup %>% group_by(GivenName,FamilyName,Sex, DOB_DY) %>%
                summarise(Count = n()) %>% 
                ungroup() %>%
                group_by(Count) %>%
                summarise(Dist = n())

```
From this quick assessment we see that for the combination of identifiers to be used we have zero (0) duplicates which is great. However, we may need to "relax" the matching criteria by limiting the number of variables used. We can see that dropping date of birth (DOB) will induce some ubiquity for potentially matching records, however use of day may be a feasible approach if we're trying to increase the sensitivity of detection.

**Review of dat2 identifiers**

```{r, echo=TRUE, warning=FALSE, message=FALSE}
# start by creating a little function to summarize and view each variable

# based on GivenName, FamilyName, Sex, and DOB
dat2a %>% group_by(GivenName,FamilyName,Sex,DOB) %>%
                summarise(Count = n()) %>% 
                ungroup() %>%
                group_by(Count) %>%
                summarise(Dist = n())
# based on GivenName, FamilyName, and Sex
dat2a %>% group_by(GivenName,FamilyName,Sex) %>%
                summarise(Count = n()) %>% 
                ungroup() %>%
                group_by(Count) %>%
                summarise(Dist = n())
# based on GivenName, FamilyName, Sex, and year
dat2a %>% group_by(GivenName,FamilyName,Sex, DOB_YR) %>%
                summarise(Count = n()) %>% 
                ungroup() %>%
                group_by(Count) %>%
                summarise(Dist = n())
# based on GivenName, FamilyName, Sex, and month
dat2a %>% group_by(GivenName,FamilyName,Sex, DOB_DY) %>%
                summarise(Count = n()) %>% 
                ungroup() %>%
                group_by(Count) %>%
                summarise(Dist = n())

```
Like we saw for dat1, for the partial identifiers to be used we have zero (0) duplicates and similar results when dropping date of birth and looking at individual components of the date.


## Deterministic matching  

It is always a good idea to do an exact match (deterministic link) on the combination of identifiers that will be used to set a baseline. This baseline will help you determine how much different (hopefully better) the linkages are by incorporating probabilistic matching. We've seen from our assessment above that using GivenName, LastName, Sex, and DOB to merge will not result in duplicate matches. 

We are going to demonstrate a simple baseline deterministic match by merging the data sets together.  

Here we merge dat1a.dedup into dat2a (right join), based on GivenName, LastName, Sex, and DOB. Because we don't want duplicate variables, aside from the ID and ID_source variables, we drop those that aren't involved with the merge.  

```{r, echo=TRUE, warning=FALSE, message=FALSE}
MergedDat <- dat1a.dedup %>%
             select(-MiddleName, -Race, -DOB_YR, -DOB_MO, -DOB_DY) %>%
             right_join(dat2a, by = c("GivenName" = "GivenName", 
                                      "FamilyName" = "FamilyName", 
                                      "Sex" = "Sex", 
                                      "DOB" = "DOB"))
```  

```{r, echo=FALSE, warning=FALSE, message=FALSE}
knitr::kable(MergedDat %>% 
             arrange(FamilyName) %>%
             head(5), booktabs = TRUE) %>%
       kableExtra::kable_styling(latex_options = c("scale_down"))
          
```  

Because we did a right join, we should retain the same number of records as we had in dat2. Let's not assume anything and check. Afterwar, we'll explore how many individuals in our dat2 did not merge with a record in dat1. From this we'll create our "linkage rate".


```{r, echo=TRUE, warning=FALSE, message=FALSE}
nrow(MergedDat) == nrow(dat2a)

# number of our sample source (dat2) that linked with a record in the 
# population source (dat1).

SumLink <- 
MergedDat %>%
  summarise(
    Linked = sum(!is.na(ID_source.x)),
    nonLinked = sum(is.na(ID_source.x))
  ) %>%
  pivot_longer(cols = everything(), names_to = "value", values_to = "count") %>%
  mutate(
    proportion = count / sum(count)
  )
as.data.frame(SumLink)

```

Using an exact match linkage (merge) the linkage "rate" was `r paste0(as.character(round(SumLink[1,3]*100,1)),"%")` 

## Probabilistic matching  
Now that we've normalized, standardized, and conducted a baseline merge, we are ready to conduct a probabilistic linkage. We are only going to go through a single approach to showcase how some of the tools work.  

The RecordLinkage package has a lot of nice features but does have some limitations. The most notable is it can be slow and require lots for memory for large data sets. The authors provide two approaches "RLBigDataLinkage" and "compare.linkage". With most public health data sets the RLBigDataLinkage() will need to be used. Because of this, although the example could use the compare.linkage we will use the former for the example.  

Another limitation is the string comparisons that are available are limited to Jaro-Winkler, Levenshtein, and Soundex. Only a single comparator can be applied (i.e, no option to apply different comparators to different variables). 

We will use the [Jaro-Winkler](https://files.eric.ed.gov/fulltext/ED325505.pdf) comparator without any blocking variables. 

### Create matched pairs
```{r, echo=TRUE, warning=FALSE, message=FALSE}
#' first create a record pairs object. We are specifying the two data sets, the 
#' string comparator, and the specifying the referential location of the columns 
#' to NOT include when considering a match. This is why it is critical that the 
#' data sets are ordered and named the same!
rpairs <- RecordLinkage:: RLBigDataLinkage(dataset1 = dat2a, 
                              dataset2 = dat1a.dedup, 
                              strcmp = TRUE, strcmpfun = "jarowinkler",
                              exclude = c(1,2,4,8,9,10,11))

# If using a really large sample, block and iterate 
# (blocking subsets by exact match): this would block on Year
## Not RUN ##
# rpairs <- RecordLinkage:: RLBigDataLinkage(dataset1 = dat2a, 
#                               dataset2 = dat1a.dedup, 
#                               strcmp = TRUE, strcmpfun = "jarowinkler",
#                               exclude = c(1,2,4,8,9,10,11),
#                               blockfld = c(9)
#                               )
```

Once the comparisons have been made using the criteria specified, we will next apply the algorithm for calculating the probability of a match. Probably the most used is the Fellegi and Sunter stochastic framework.  The RecordLinkage package however, also provides the EpiLink and EM algorighms (see [package documentation](https://cran.r-project.org/web/packages/RecordLinkage/RecordLinkage.pdf) for details).

The selected cutoff level depends on the algorithm choice, type of comparator, and level of desired sensitivity/specificity.   

For this example, we'll use the EpiLink algorithm.
```{r, echo=TRUE, warning=FALSE, message=FALSE}
# create weights for each record pair
rpairs <- RecordLinkage::epiWeights(rpairs) 
print(rpairs)
# wts_fs <- RecordLinkage::fsWeights(rpairs)  #Fellegi and Sunter (pair with fsClassify)
# wts_eu <- RecordLinkage::emWeights(rpairs)  #M and U probabillity assessment 
#           using the EM algorithm (pair with emClassify)
```

The respective weight functions calculate the probability of a match for each record in data set 1 with each record in data set 2 based on the comparator patterns specified. We need to classify the weights as links, potential links, or non-links. To make these classifications we need to establish our acceptance threshold. The threshold, is the probability level that a  above would be accepted as a link or below would be rejected. The threshold my also be a region where above would be accepted as a link, within would be potential, and below would be rejected. The area of uncertainty (potential matches) requires some sort of reconciliation (record review).   

As indicated above, choosing the thresholds is usually done through record review and classification. One of the tools in the RecordLinkage package is the getParetoThreshold() function, which can also be useful for identifying acceptance thresholds. However, with large data sets it can take a long time to run. It is often best to run it a few times with samples taken from the data or a smaller training data set. 

```{r, echo=TRUE, warning=FALSE, message=FALSE}
# due to the time it takes to run, the function has been commented out and 
# replaced with the result after running it once.

# pth <- RecordLinkage::getParetoThreshold(rpairs,interval=c(0.75, 1.0)) 
  pth <- 0.875

# note you can leave out the interval statement to choose the threshold from a plot. 
# RecordLinkage::getParetoThreshold(rpairs) 

```

Based on the image from the getParetoThreshold() (see below) the long flat arm suggesting that it will be unlikely to identify correct matches below a weight of around 0.8. Based on the interval selected the suggested automatic acceptance threshold is `r pth`.

```{r echo=FALSE, fig.alt="Image of Mean Residual Life (MRL) on y-axis, and threshold level.", out.width = '100%'}
knitr::include_graphics(
  "C:/Users/parri/Documents/R_Code/RecordLinkage/R_Linkage/image/Rplot_GPT.png"
  )
```
  
### Classify weights  

Now we'll use the information and classify linkages. According to the RecordLinkage documentation for the epiClassify() function. "All record pairs with weights greater or equal threshold.upper are classified as links. Record pairs with weights smaller than threshold.upper and greater or equal threshold.lower are classified as possible links. All remaining records are classified as non-links." 


**Classify based on the ParetoThreshold**
```{r, echo=TRUE, warning=FALSE, message=FALSE}
# First classify links based on the ParetroThreshold
eclsf <- RecordLinkage::epiClassify(rpairs, pth)
# Next classify and return pairs with a probability between 0.75 and 1.0
RlMtch <- RecordLinkage::getPairs(
  eclsf, min.weight=0.75, max.weight=1, single.row=FALSE
  )
# Finally, update is_match based on the Class
RlMtch$is_match <- with(RlMtch, ifelse(is.na(is_match) & Class=="L",1,""))
```

Using the ParetoTHreshold() as an automatic acceptance alone will likely result in poor results. As we can see. The linkages "L" in the table below suggest 389 linkages, when we only have 302 records. This is incorrect and leads to false positive matches. However, it is a great starting point for considering what levels will optimize manual review and minimize the number of reviews needed. 
```{r, echo=TRUE, warning=FALSE, message=FALSE}
table(RlMtch$Class,useNA = "always")
```

**Classify informed by the ParetoThreshold with manual review**
Now we'll specify intervals. We'll automatically accept those with a probability of 1.0 and then review those with a probability between 0.8 and 0.99.  
```{r, echo=TRUE, warning=FALSE, message=FALSE}
# First classify links based on the ParetroThreshold
eclsf_1 <- RecordLinkage::epiClassify(
  rpairs, threshold.upper = 1.0, threshold.lower = 0.75
  )
# Next classify and return pairs with a probability between 0.75 and 1.0
RlMtch_1 <- RecordLinkage::getPairs(
  eclsf_1, min.weight=0.75, max.weight=1, single.row=FALSE
  )
# Finally, update is_match based on the Class.
RlMtch_1$is_match <- with(RlMtch_1, ifelse(is.na(is_match) & Class=="L",1,""))
```

Lets look at the numbers that linked and possible matches that need review by probability score.  

```{r, echo=TRUE, warning=FALSE, message=FALSE, fig.width=10,fig.height=5}
table(RlMtch_1$Class)
# create bins for the probability scores (weights)
RlMtch_1$cutpnt <- with(RlMtch_1, cut(round(as.numeric(Weight),2), 
                                      breaks=(seq(0.75,1, by = .01)), 
                                      include.lowest = TRUE, right = TRUE), "")
# print the counts by bin
table(RlMtch_1$cutpnt)
# print the counts by bin by matching status (L = Link, N = nonLink, P = possibleLink)
table(RlMtch_1$cutpnt,RlMtch_1$Class,useNA = "always")

# Plot histogram of counts by bin for possible matches

RlMtch_1 %>% 
         filter(Class == "P") %>%
         group_by(cutpnt) %>%
         summarize(count = n()) %>%
         ggplot() +
           geom_col(aes(x = cutpnt, y = count)) +
           theme(axis.text.x = element_text(angle = 30, vjust = 0.95, hjust = 1)) +
           xlab("Probability match bin") +
           ylab("Count of possible matches")
```

From the histogram and table, it is easy to see how quickly the amount of manual review can balloon to where thousands of records need reconciled. For this example, we will conduct manual reviews to 0.85 and then proceed to lower match probability bins. We'll stop once we've reviewed two lower bins with no successful matches. We will then assume all lower matches are non-matches. 

Depending on the goal of the linkage the purpose of manual review may be different. A manual review may be conducted to establish the automatic acceptance level and quantify the error. It may also be a part of the normal linkage process where specificity of the linkages is critical.  

To conduct manual review a couple options are available. You can  use the base r edit() function, export the file as a csv or excel file conduct the review and import back in, or track and update with r code based on the row reference. The editMatch() function in the RecordLinkage package  opens up the r data editor and is typically used for creating minimal training datasets. 

Depending on your preference the getPairs() function can present the pairs in a single row "getPairs(..., single.row=TRUE)" or multiple rows "getPairs(..., single.row=FALSE)". It is generally visually easier to work with the multiple rows for reviewing but does require more data wrangling once completed. This exercise will present one way of working with the multiple rows for review.  
If using the edit() in R to directly modify the underlying data it is recommended to save your work as a new object (especially if you will not be finishing the review in a single sitting).  

We typically use the is_match field and set 1 = match, 2 = needs additional review, 3 = nonmatch, and leave blank for those not reviewed. With the multi-row view we only update the second record of the presented match set.

Below is a screen shot of the edit() indicating how we update using the basic editor. For large projects however, we save the pairs as a csv file, conduct the manual review and then load the reviewed file for finalization.  

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Example of using the R editor.
# mlr <- edit(RlMtch_1) 

# Saving a csv file and conducting manual review externally. 
# write.csv(RlMtch_1, "ManualReviewMatch.csv", row.names=FALSE)
```

```{r echo=FALSE, fig.alt="Image of potential matches from the r base editor.", out.width = '100%'}
knitr::include_graphics(
  "C:/Users/parri/Documents/R_Code/RecordLinkage/R_Linkage/image/ML_edit.png"
  )
```

As a side note: make sure you develop a set of rules for your manual review that can be replicated. For the manual review in this exercise the following rules for matches were followed:  
1. Character(s) change in variable(s) that do not result in a known valid spelling of the name(s).
2. A single change in date of birth year, month, or day (exception for 12/31 and 1/1 transpositions +/- 5 days).
3. A difference in sex specification with all other variables matching.
4. Character(s) change in variables(s) that result in a valid name, but a valid middle name is the same.
5. All other non-discernible potential matches reviewed by two people and classified.  

### Summarize linkage  

After the manual review we can summarize the review findings.  

```{r echo=TRUE, message=FALSE, warning=FALSE}
# load manual reviewed data
# Rev_pairs <- read.csv("ManualReviewMatch.csv") if ran locally. 
# retrieve from Github
git3 <- 'https://raw.githubusercontent.com/parrish-epi/R-recordLinkage/main/ManualReviewMatch.csv' 
Rev_pairs <-read.csv(git3, fileEncoding = "ISO-8859-1") 
rm(git3)
## re-run the cutpnt code to ensure we have an ordered factor
Rev_pairs$cutpnt <- with(Rev_pairs, cut(round(as.numeric(Weight),2), 
                                      breaks=(seq(0.75,1, by = .01)), 
                                      include.lowest = TRUE, right = TRUE), "")
# summarize links by weight bin.

addmargins(table(Rev_pairs$cutpnt,Rev_pairs$is_match))

```

Next, let's figure out the linkage match % for each bin and then cumulatively.

```{r echo=TRUE, message=FALSE, warning=FALSE}
kable(
Rev_pairs %>% filter(!is.na(is_match)) %>%
              group_by(cutpnt,is_match) %>% 
              summarise(Count = n()) %>%
              ungroup() %>%
              pivot_wider(names_from = is_match, values_from = Count, values_fill = 0) %>%
              mutate(total =`3` + `1`) %>%
              mutate(PropLink = `1`/total,) %>%
              rename(nonLink = `3`, Link = `1`) %>%
              arrange(desc(as.numeric(row.names(.)))) %>%
              mutate(Clink = cumsum(Link),
                     Ctotal = cumsum(total)) %>%
              mutate(cPropLink = Clink/Ctotal)
)

```

From this, we can see we detected fewer matches at the lower match probabilities. If we were going to be developing an automatic acceptance threshold, this would be much more informative than relying only on the ParetoThreshold calculated above. For example, if I decided to set a threshold at >0.95, 95% of the record pairs in this range would be expected to be true matches. 

To finish this off and calculate our linkage rates for dat2 (our sample), which is the proportion that successfully matched with the dat1 (population source), we need to merge our linked data to the full data.  To do this we need to do a little bit of data wrangling. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
# remove blank rows 
Matched_pairs <- Rev_pairs %>% filter(!is.na(id)) 
# split into two dataframes
MP_1 <- Matched_pairs %>% filter(as.numeric(row.names(.)) %% 2!=0)
MP_2 <- Matched_pairs %>% filter(as.numeric(row.names(.)) %% 2==0)
## verify that they have the same number of records AND DO NOT sort
nrow(MP_1)
nrow(MP_2)

## combine the ID's and include the matching criteria contained in MP_2
MatchedPairsFinal <- cbind(MP_2[,c(3,13:16)], MP_1[,3])

# First Remove blank rows
Matched_pairs <- Rev_pairs %>% filter(!is.na(id))

# Add a row index to keep track of the pairs
Matched_pairs <- Matched_pairs %>% mutate(row_index = row_number())

# Separate the data into two data frames
MP_1 <- Matched_pairs %>% filter(row_index %% 2 != 0)
MP_2 <- Matched_pairs %>% filter(row_index %% 2 == 0)

# Verify that they have the same number of records AND DO NOT sort
if (nrow(MP_1) == nrow(MP_2)) {
  # Combine the ID's and include the matching criteria contained in MP_2
  MatchedPairsFinal <- MP_2 %>%
    select(ID_source, is_match, Class, Weight, cutpnt) %>%
    rename(ID_source1 = "ID_source") %>%
    bind_cols(MP_1 %>%
                select(ID_source) %>%
                rename(ID_source2 = "ID_source")
             ) %>%
    filter(is_match == 1)
} else {
  stop("The two data frames do not have the same number of records")
}

```

Now that we have the data in a nice format, before we merge back to our original sample (dat2), we need to check if we created any duplicates. This can be done at different times in the linkage process, but for simplicity we're just doing it once here.

**Check for duplicated records**
```{r echo=TRUE, message=FALSE, warning=FALSE}
table(duplicated(MatchedPairsFinal$ID_source1))
```

This is great no duplicates. This means we didn't link an individual twice. If we had, a simple strategy would be to retain the match with the highest probability match score.  

Let's now merge the linked data back to dat2 (our sample) and calculate the linkage rate.  

```{r echo=TRUE, message=FALSE, warning=FALSE}
FinalMatchedData <- merge(dat2, MatchedPairsFinal, 
                          by.x = "ID_source",
                          by.y = "ID_source2",
                          all.x = TRUE)

# create a summary table of the linked data
SumLinkT <- 
FinalMatchedData %>%
    summarise(
       Linked = sum(!is.na(ID_source1)),
       nonLinked = sum(is.na(ID_source1))
  ) %>%
  pivot_longer(cols = everything(), names_to = "value", values_to = "count") %>%
  mutate(
    proportion = count / sum(count)
  )
as.data.frame(SumLinkT)

```

With manual review based on our probability scores we returned a linkage 'rate' of `r paste0(as.character(round(SumLinkT[1,3]*100,1)),"%")` compared to `r paste0(as.character(round(SumLink[1,3]*100,1)),"%")`  using a deterministic exact match. If we were feeling particularly ambitious, we could inspect the remaining nonlinked records or conduct a second round of linkages with the remaining records using a different, relaxed set of criteria.  


# Conclusion  

In this example, we demonstrated the use of the RecordLinkage() package in R to perform record linkage a crucial process for identifying and merging duplicate records across data sets. We started by loading and pre-processing the data, ensuring it was clean and standardized. We then created a record linkage object, specifying the fields for comparison and string comparator to enhance efficiency. By computing comparison weights and classifying the matches, we were able to identify and review potential matches between records from different data sets. This example serves as a basic approach, showcasing the essential steps and functions of the RecordLinkage package, and can be extended to more complex scenarios and data sets as needed.

---

<!-- <div class="footer"> -->
<!-- ```{r, echo=FALSE, fig.alt="ASTHO logo.", out.width='100px'} -->
<!-- knitr::include_graphics("image/RSImage-329695.png") -->
<!-- ```  -->
<!-- </div> -->

<div class="footer">

  <!-- Logo image with link -->
  <a href="https://www.astho.org">
        ```{r, echo=FALSE, fig.alt="ASTHO logo.", out.width='100px'}
        knitr::include_graphics("image/RSImage-329695.png") 
        ```
  <br>
  </a>
  <!-- About link to open pop-up -->
    <a href="#" id="aboutLink">About</a>
  <!-- Hidden pop-up content -->
  <div id="aboutPopup" class="popup">
  <div class="popup-content">
  <span class="close" onclick="closeAboutPopup()">&times;</span>  
  
  **Created By:**  
  Jared Parrish, PhD   
  Parrish Analytics and Epidemiology Consulting  
  email: <a href="mailto:jared.parrish@alaska.gov">jared.parrish@alaska.gov</a>  
  URL: <a href="https://github.com/parrish-epi/R-recordLinkage">https://github.com/parrish-epi/R-recordLinkage</a>  
  </div>
 </div>
</div>

<script>

// Function to open the About pop-up
function openAboutPopup() {
    document.getElementById('aboutPopup').style.display = 'flex'; // Display as flex to center content
}

// Function to close the About pop-up
function closeAboutPopup() {
    document.getElementById('aboutPopup').style.display = 'none';
}

// Event listener to trigger pop-up when clicking on the About link
document.getElementById('aboutLink').addEventListener('click', function(event) {
    event.preventDefault(); // Prevent default anchor behavior
    openAboutPopup();
});
  
</script>
